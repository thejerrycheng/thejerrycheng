# **Jerry (Qilong) Cheng**

I build intelligent robotic systems that unify **perception, locomotion, and manipulation**.
My work spans **reinforcement learning**, **humanoid & exoskeleton control**, and **robot/mechatronic design** (FOC actuators, differential drives, 6-DOF cinema arms).
Currently a **Ph.D. student in Robotics at NYU**.

---

## **About Me**

I work on **RL for humanoid locomotion and dexterous manipulation**, with prior research in:

* **State estimation** (UTIAS STARS Lab — Prof. Jonathan Kelly)
* **Haptics & HRI** (UofT DGP Lab — Prof. Daniel Wigdor)
* **Learning-based manipulation & control** (UofT Robotics — Prof. Matthew Mackay, Prof. Ali Bereyhi)

Across labs, my focus has been consistent:
**building embodied controllers that merge perception, dynamics, and contact into unified, deployable systems.**

---

## **Research Focus**

* **Unified RL for locomotion & loco-manipulation**
  Humanoids, exoskeletons, and rockets with contact-rich, torque-level control.

* **Model-based × data-driven control**
  MoE policies, dynamics-aware RL, and hybrid controllers that blend physics with learning.

* **Embodied perception for control**
  Multimodal sensing (vision, tactile, proprioception), monocular depth, and certifiable calibration pipelines.

* **Robot & actuator design**
  FOC actuators, differential wrist mechanisms, lightweight cinema robot arms, and hardware–software co-design.

---

## **Selected Projects**

* **IRIS** — lightweight 6-DOF cinema robot arm
  (custom QDD actuators, carbon-fiber links, differential wrist)
* **MoE-based RL controller** for adaptive hip exoskeletons
  (full musculoskeletal simulation + RL adaptation)
* **Globally optimal radar–camera calibration**
  (IJRR 2025 — certifiable RWHE calibration)
* **AeroHaptix** — spatialized vibrotactile system for teleoperation & HCI

---

## **Research Vision**

I aim to integrate **UMI-scale multimodal perception** with **torque-level RL**, seeking to answer:

* How do we fuse **foundation models** with **dynamics-aware control**?
* How can vision, tactile, and proprioception be unified in a **single embodied policy**?
* How do **IL and RL** merge to generalize beyond demonstrations?
* How can we co-design **hardware, sensing, and control** to make learning systems truly scalable?

I strongly believe in Steve Jobs’ vision that
**“those who are serious about software should design their own hardware.”**
